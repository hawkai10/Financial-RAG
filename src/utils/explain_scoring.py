#!/usr/bin/env python3
"""
Comprehensive explanation and demonstration of RAG scoring system
"""
import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

from txtai import Embeddings
from rag_backend import rag_query_enhanced

def explain_scoring_system():
    print("=" * 80)
    print("üîç RAG SCORING SYSTEM EXPLAINED")
    print("=" * 80)
    
    print("""
The scoring in your RAG system happens in multiple stages:

1Ô∏è‚É£  EMBEDDING SIMILARITY SCORES (0.0 to 1.0)
   ‚Ä¢ Generated by txtai using sentence-transformers model
   ‚Ä¢ Cosine similarity between query and chunk embeddings
   ‚Ä¢ Higher = more similar (0.6+ is good, 0.5+ is decent)
   ‚Ä¢ Range: 0.0 (completely different) to 1.0 (identical)

2Ô∏è‚É£  CROSS-ENCODER RERANKING SCORES (can be negative)
   ‚Ä¢ Uses a different model to rerank the initial results
   ‚Ä¢ More sophisticated than simple embedding similarity
   ‚Ä¢ Can be negative (lower relevance) or positive (higher relevance)
   ‚Ä¢ Final scores used for chunk ordering

3Ô∏è‚É£  HYBRID SEARCH SCORES (CURRENTLY ENABLED)
   ‚Ä¢ Combines embedding similarity + BM25 keyword matching
   ‚Ä¢ Uses BM25Okapi algorithm for sparse retrieval
   ‚Ä¢ Weighted average: 70% semantic + 30% keyword (HYBRID_ALPHA=0.7)
   ‚Ä¢ BM25 parameters: k1=1.2, b=0.75
   
Let's see this in action:
""")
    
    try:
        # Load embeddings for direct testing
        print("Loading embeddings for direct testing...")
        embeddings = Embeddings({'path': 'sentence-transformers/all-MiniLM-L6-v2', 'content': True})
        embeddings.load('business-docs-index')
        
        # Test query
        query = "what is the rent for second year"
        print(f"\nüîç QUERY: '{query}'")
        
        print("\n" + "-" * 60)
        print("1Ô∏è‚É£  INITIAL EMBEDDING SIMILARITY SCORES")
        print("-" * 60)
        
        initial_results = embeddings.search(query, 5)
        for i, result in enumerate(initial_results):
            score = result['score']
            chunk_id = result['id'].split('\\')[-1]  # Just filename
            text_preview = result.get('text', '')[:100]
            
            # Determine quality level
            if score >= 0.6:
                quality = "üü¢ EXCELLENT"
            elif score >= 0.5:
                quality = "üü° GOOD"
            elif score >= 0.4:
                quality = "üü† FAIR"
            else:
                quality = "üî¥ POOR"
                
            print(f"{i+1}. Score: {score:.3f} {quality}")
            print(f"   ID: {chunk_id}")
            print(f"   Text: {text_preview}...")
            print()
        
        print("\n" + "-" * 60)
        print("2Ô∏è‚É£  FULL RAG PIPELINE WITH RERANKING")
        print("-" * 60)
        
        # Run full RAG pipeline
        rag_result = rag_query_enhanced(
            question=query,
            embeddings=embeddings,
            topn=5,
            enable_reranking=True
        )
        
        chunks = rag_result.get('chunks', [])
        print(f"Final chunks after reranking: {len(chunks)}")
        
        for i, chunk in enumerate(chunks):
            # Get different score types
            initial_score = chunk.get('retrieval_score', 'N/A')
            rerank_score = chunk.get('final_rerank_score', 'N/A')
            combined_score = chunk.get('combined_score', 'N/A')
            
            text_preview = chunk.get('text', chunk.get('chunk_text', ''))[:100]
            chunk_id = chunk.get('chunk_id', 'Unknown').split('\\')[-1]
            
            print(f"{i+1}. CHUNK: {chunk_id}")
            print(f"   Initial Score:  {initial_score}")
            print(f"   Rerank Score:   {rerank_score}")
            print(f"   Combined Score: {combined_score}")
            print(f"   Text: {text_preview}...")
            print()
            
        print("\n" + "=" * 60)
        print("üìä SCORING INTERPRETATION GUIDE")
        print("=" * 60)
        print("""
EMBEDDING SIMILARITY SCORES:
‚Ä¢ 0.7+ = Highly relevant (rare, excellent match)
‚Ä¢ 0.6+ = Very relevant (good semantic match)  
‚Ä¢ 0.5+ = Moderately relevant (decent match)
‚Ä¢ 0.4+ = Somewhat relevant (may contain useful info)
‚Ä¢ <0.4 = Likely not relevant

CROSS-ENCODER RERANKING SCORES:
‚Ä¢ Positive scores = Higher relevance than average
‚Ä¢ Scores near 0 = Average relevance
‚Ä¢ Negative scores = Lower relevance than average
‚Ä¢ Very negative (-5+) = Poor relevance

WHY SCORES MATTER:
‚Ä¢ Initial scores determine which chunks are retrieved
‚Ä¢ Reranking scores determine final order sent to LLM
‚Ä¢ Your system now retrieves top 4-6 chunks instead of 3
‚Ä¢ This captures more potentially relevant content
""")
        
    except Exception as e:
        print(f"ERROR: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    explain_scoring_system()
